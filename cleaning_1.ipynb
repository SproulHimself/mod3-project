{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onion_one import onion_articles \n",
    "from satirewire_data import satire_wire_articles\n",
    "from squib_data import squib_1_articles\n",
    "from squib_2_data import squib_2_articles \n",
    "import re\n",
    "import pdb\n",
    "import pandas as pd \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "324\n",
      "180\n",
      "699\n"
     ]
    }
   ],
   "source": [
    "print(len(onion_articles))\n",
    "print(len(satire_wire_articles))\n",
    "print(len(squib_1_articles))\n",
    "print(len(squib_2_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "onion_arts = [l.pop(0) for l in onion_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(onion_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_onions = [x for x in onion_articles if x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "onion_df = pd.DataFrame(cleaned_onions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nation Not Sure How Many Ex-Trump Staffers It ...</td>\n",
       "      <td>Noting that the resignation of James Mattis as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exhausted Robert Mueller Turns Off Phone To Gi...</td>\n",
       "      <td>Desperate to unwind after months of nonstop wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Trump Presidency In 2018</td>\n",
       "      <td>Nearly halfway through his presidential term, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ryan Zinke Apologizes For Misuse Of Government...</td>\n",
       "      <td>Attempting to make amends for gross abuses of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump Administration Launches Human Rights Inv...</td>\n",
       "      <td>Decrying the Senate’s resolution blaming the c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  Nation Not Sure How Many Ex-Trump Staffers It ...   \n",
       "1  Exhausted Robert Mueller Turns Off Phone To Gi...   \n",
       "2                       The Trump Presidency In 2018   \n",
       "3  Ryan Zinke Apologizes For Misuse Of Government...   \n",
       "4  Trump Administration Launches Human Rights Inv...   \n",
       "\n",
       "                                                body  \n",
       "0  Noting that the resignation of James Mattis as...  \n",
       "1  Desperate to unwind after months of nonstop wo...  \n",
       "2  Nearly halfway through his presidential term, ...  \n",
       "3  Attempting to make amends for gross abuses of ...  \n",
       "4  Decrying the Senate’s resolution blaming the c...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_case = onion_df.iloc[4][1]\n",
    "onion_df.columns = ['headline', 'body']\n",
    "onion_df.head() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_footer(dirty_string): \n",
    "    sep = \"SatireWire.com Related Tags:\"\n",
    "    if sep in dirty_string:\n",
    "        clean = dirty_string.split(sep, 1)[0]\n",
    "    else:\n",
    "        clean = dirty_string\n",
    "    return clean \n",
    "\n",
    "# onion_df[1] = onion_df[1].map(remove_dates)\n",
    "# onion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "squib_df = pd.DataFrame(squib_articles)\n",
    "# squib_df.head() \n",
    "# squib_df.iloc[3][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALTERNATIVE PUERTO RICO THRIVING AFTER HURRICA...</td>\n",
       "      <td>White House officials today said President Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MORNING SHOW ‘FUCK NO, TALLAHASSEE’ TOPS TV RA...</td>\n",
       "      <td>Just one month after changing its name from ‘...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRUMP TO KEEP FAMILIES TOGETHER, STACKED INTO ...</td>\n",
       "      <td>Official White House Transcript Topic: Immigra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRUMP CANCELS 2018 MIDTERMS TO THWART RUSSIAN ...</td>\n",
       "      <td>In an effort to keep Russia from interfering ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRUMP WILL DEPLOY RUSSIAN TROOPS TO PURGE U.S....</td>\n",
       "      <td>President Donald Trump today accepted an offe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  ALTERNATIVE PUERTO RICO THRIVING AFTER HURRICA...   \n",
       "1  MORNING SHOW ‘FUCK NO, TALLAHASSEE’ TOPS TV RA...   \n",
       "2  TRUMP TO KEEP FAMILIES TOGETHER, STACKED INTO ...   \n",
       "3  TRUMP CANCELS 2018 MIDTERMS TO THWART RUSSIAN ...   \n",
       "4  TRUMP WILL DEPLOY RUSSIAN TROOPS TO PURGE U.S....   \n",
       "\n",
       "                                                body  \n",
       "0   White House officials today said President Tr...  \n",
       "1   Just one month after changing its name from ‘...  \n",
       "2  Official White House Transcript Topic: Immigra...  \n",
       "3   In an effort to keep Russia from interfering ...  \n",
       "4   President Donald Trump today accepted an offe...  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# satire_pd = pd.DataFrame(satire_wire_articles)\n",
    "# satire_pd.drop([0], axis=1, inplace=True)\n",
    "# # satire_pd.head() \n",
    "# satire_pd.iloc[4][2]\n",
    "# satire_pd[2] = satire_pd[2].map(remove_footer)\n",
    "# # satire_pd.head()\n",
    "# satire_pd.iloc[10][2]\n",
    "satire_pd.columns = ['headline', 'body']\n",
    "satire_pd.head() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1230, 2)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# squib_df.head()\n",
    "# onion_df.head()\n",
    "# satire_pd\n",
    "# all_satire_df = onion_df.append([squib_df, satire_pd])\n",
    "# all_satire_df = all_satire_df.drop_duplicates()\n",
    "all_satire_df['target'] = 1\n",
    "all_satire_df.head() \n",
    "all_satire_b = all_satire_df.drop(['headline'], axis=1)\n",
    "all_satire_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Obama Oprah O’Rourke Ocasio – Why Socialist Am...</td>\n",
       "      <td>O socialism America Oprah Obama O'Rourke Ocasi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Golden Globes Red Carpet Extravaganza Wows Crowds</td>\n",
       "      <td>“Did you see those golden globes?” an excited ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Modigliani Nude Paintings Now Considered as Se...</td>\n",
       "      <td>Amedeo Modigliani (1884–1920) The ever present...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Doesn’t Matter What Year We’re In Still Have t...</td>\n",
       "      <td>If you think another year after the previous y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why No One Will Ever Vote For the Conservative...</td>\n",
       "      <td>Theresa May. It’s as simple as that. She and h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  Obama Oprah O’Rourke Ocasio – Why Socialist Am...   \n",
       "1  Golden Globes Red Carpet Extravaganza Wows Crowds   \n",
       "2  Modigliani Nude Paintings Now Considered as Se...   \n",
       "3  Doesn’t Matter What Year We’re In Still Have t...   \n",
       "4  Why No One Will Ever Vote For the Conservative...   \n",
       "\n",
       "                                                body  \n",
       "0  O socialism America Oprah Obama O'Rourke Ocasi...  \n",
       "1  “Did you see those golden globes?” an excited ...  \n",
       "2  Amedeo Modigliani (1884–1920) The ever present...  \n",
       "3  If you think another year after the previous y...  \n",
       "4  Theresa May. It’s as simple as that. She and h...  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squib_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "squib_articles = squib_1_articles + squib_2_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "879"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(squib_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "629+47+324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.tokenize import word_tokenize\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_satire_articles = squib_articles + onion_articles + satire_wire_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Obama Oprah O’Rourke Ocasio – Why Socialist Americans Love ‘O’ So Much',\n",
       " \"O socialism America Oprah Obama O'Rourke Ocasio Michelle Obama, Barack Obama, Oprah Winfrey, Beto O‘Rourke, Ocasio-Cortez, these are all darlings of the socialist democrat party in America who all have something in common, the letter ‘o’. What is it about the letter ‘o’ that is so endearing to the socialists and their dream of turning America into a pseudo-communist socialist state resembling Venezuela? We asked Democrat party official Oona Menendez, the same question. “Why is the letter ‘o’ so important for Democrat politicians, especially regarding elections?” She replied quite succinctly: “Candidates have a greater success with the letter ‘o’ in their name because it breeds familiarity. Like with an ‘o’ people immediately know they’re safe, they are guaranteed socialism or communism. We adopted the significance of ‘o’ after eight years of Barack Obama rule. He was the first big ‘o’ which denoted ‘change’ and he brought in much socialist and Marxist changes to America, which is sadly being eroded by the current president. ‘O’ also stands for the circle, inclusivity, globalism, and collectivism. The main reason though, as I mentioned before, is familiarity, because when a voter is about to tick the box on election day, all they have to look for is where the Os are. If you see an ‘o’ on the first or last name, that’s who you vote for — Democrat. It’s really as simple as that.” We asked her where ‘Hillary’ fits into this? “She doesn’t. Never has, never will.” \"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onion_articles[0][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Swimming enthusiast Yu Hongtao, 50, trained f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>North Korean leader Kim Jong Un and Chinese P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indonesia will launch a renewed search effort...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Myanmar government leader Aung San Suu Kyi di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>France plans to introduce legislation to toug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  target\n",
       "0   Swimming enthusiast Yu Hongtao, 50, trained f...       0\n",
       "1   North Korean leader Kim Jong Un and Chinese P...       0\n",
       "2   Indonesia will launch a renewed search effort...       0\n",
       "3   Myanmar government leader Aung San Suu Kyi di...       0\n",
       "4   France plans to introduce legislation to toug...       0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_df = pd.read_csv('reuters_clean.csv')\n",
    "reuters_df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "reuters_df.head() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_data = all_satire_b.append(reuters_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2128, 2)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mega_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Noting that the resignation of James Mattis as Secretary of Defense marked the ouster of the third top administration official in less than three weeks, a worried populace told reporters Friday that it was unsure how many former Trump staffers it could safely reabsorb. “Jesus, we can’t just take back these assholes all at once—we need time to process one before we get the next,” said 53-year-old Gregory Birch of Naperville, IL echoing the concerns of 323 million Americans in also noting that the country was only now truly beginning to reintegrate former national security advisor Michael Flynn. “This is'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test = mega_data.iloc[0][0]\n",
    "' '.join(text_test.split()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_body(string):\n",
    "    return ' '.join(text_test.split()[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2128, 2)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reindexed.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reindexed.to_csv(\"clean_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_reindexed['body']\n",
    "target = data_reindexed['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = mega_data.reset_index()\n",
    "data_reindexed = data2.drop(columns ='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_list = stopwords.words('english')\n",
    "sw_list += list(string.punctuation)\n",
    "sw_list += [\"''\", '\"\"', '...', '``', '’', '“', '’', '”', '‘', '‘', 'said', 'one']\n",
    "sw_set = set(sw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_article(article):\n",
    "    tokens = nltk.word_tokenize(article)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in sw_set]\n",
    "    return stopwords_removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = list(map(process_article, data))\n",
    "# tokens = nltk.word_tokenize(data[0])\n",
    "# removed = []\n",
    "# for token in tokens: \n",
    "#     if token.lower() not in sw_set: \n",
    "#         removed.append(token.lower())\n",
    "# removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42429"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_vocab = set()\n",
    "for comment in processed_data:\n",
    "    total_vocab.update(comment)\n",
    "len(total_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles_concat = []\n",
    "# for article in processed_data:\n",
    "#     articles_concat += article\n",
    "def join_list(list): \n",
    "    ' '.join(list)\n",
    "        \n",
    "X = [' '.join(d) for d in processed_data]\n",
    "y = target\n",
    "# X[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles_freqdist = FreqDist(articles_concat)\n",
    "# articles_freqdist.most_common(100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1702x32647 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 283257 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tf_idf_data_train = vectorizer.fit_transform(X_train)\n",
    "tf_idf_data_test = vectorizer.transform(X_test)\n",
    "# article_vector = vectorizer.transform([article])\n",
    "tf_idf_data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x32647 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 179 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_data_test[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Number of Non-Zero Elements in Vectorized Articles: 166.42596944770858\n",
      "Percentage of columns containing 0: 0.9949022584173827\n"
     ]
    }
   ],
   "source": [
    "non_zero_cols = tf_idf_data_train.nnz / float(tf_idf_data_train.shape[0])\n",
    "print(\"Average Number of Non-Zero Elements in Vectorized Articles: {}\".format(non_zero_cols))\n",
    "\n",
    "percent_sparse = 1 - (non_zero_cols / float(tf_idf_data_train.shape[1]))\n",
    "print('Percentage of columns containing 0: {}'.format(percent_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier.fit(tf_idf_data_train, y_train)\n",
    "nb_train_preds = nb_classifier.predict(tf_idf_data_train)\n",
    "nb_test_preds = nb_classifier.predict(tf_idf_data_test)\n",
    "chad_predict = nb_classifier.predict(article_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chad_predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier.fit(tf_idf_data_train, y_train)\n",
    "rf_train_preds = rf_classifier.predict(tf_idf_data_train)\n",
    "rf_test_preds = rf_classifier.predict(tf_idf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_score = accuracy_score(y_train, nb_train_preds)\n",
    "nb_test_score = accuracy_score(y_test, nb_test_preds)\n",
    "rf_train_score = accuracy_score(y_train, rf_train_preds)\n",
    "rf_test_score = accuracy_score(y_test, rf_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes\n",
      "Training Accuracy: 0.9906 \t\t Testing Accuracy: 0.9695\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Random Forest\n",
      "Training Accuracy: 1.0 \t\t Testing Accuracy: 0.9812\n"
     ]
    }
   ],
   "source": [
    "# so we have some really great results here. \n",
    "# maybe we are overfitting? Or maybe its really easy to determine satire? \n",
    "\n",
    "print(\"Multinomial Naive Bayes\")\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(nb_train_score, nb_test_score))\n",
    "print(\"\")\n",
    "print('-'*70)\n",
    "print(\"\")\n",
    "print('Random Forest')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(rf_train_score, rf_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process sample helper function here \n",
    "\n",
    "def sample_prep(body): \n",
    "    article = ' '.join(process_article(body))\n",
    "    return vectorizer.transform([article])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign a string to the body variable and see if its \n",
    "# satire or not \n",
    "# I have tried it with NY times and some other satire and its worked. \n",
    "\n",
    "body = \"Leaders like Jonny Boucher, a Chicago native who, after losing too many friends and family to suicide, started a coffee shop to offer emotional support and guidance to those who might be in need of a little more than a strong cup of coffee to get through their day.\"\n",
    "\n",
    "\n",
    "article_vector = sample_prep(body)\n",
    "nb_classifier.predict(article_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_grid = {\n",
    "    'n_estimators': [10, 30, 100],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 2, 6, 10],\n",
    "    'min_samples_split': [10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "import time\n",
    "start = time.time()\n",
    "rf_grid_search = GridSearchCV(rf_clf, rf_param_grid, cv=3)\n",
    "rf_grid_search.fit(scaled_df, labels)\n",
    "\n",
    "print(\"Testing Accuracy: {:.4}%\".format(rf_grid_search.best_score_ * 100))\n",
    "print(\"Total Runtime for Grid Search on Random Forest Classifier: {:.4} seconds\".format(time.time() - start))\n",
    "print(\"\")\n",
    "print(\"Optimal Parameters: {}\".format(rf_grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier()\n",
    "mean_rf_cv_score = np.mean(cross_val_score(rf_clf, scaled_df, labels, cv=3))\n",
    "\n",
    "print(\"Mean Cross Validation Score for Random Forest Classifier: {:.4}%\".format(mean_rf_cv_score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
